{"cells":[{"cell_type":"markdown","id":"75b58048-7d14-4fc6-8085-1fc08c81b4a6","metadata":{"id":"75b58048-7d14-4fc6-8085-1fc08c81b4a6"},"source":["# Fine-Tune Whisperfollowing \n","The code is inspired and adapted from the following blogpost:\n","https://huggingface.co/blog/fine-tune-whisper"]},{"cell_type":"code","execution_count":1,"id":"95048026-a3b7-43f0-a274-1bad65e407b4","metadata":{"id":"95048026-a3b7-43f0-a274-1bad65e407b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue Apr 30 12:54:19 2024       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.67                 Driver Version: 550.67         CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA GeForce RTX 3060 ...    Off |   00000000:01:00.0 Off |                  N/A |\n","| N/A   53C    P8             10W /   80W |       8MiB /   6144MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|    0   N/A  N/A     47155      G   /usr/bin/gnome-shell                            2MiB |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["import os\n","import pickle\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":2,"id":"e68ea9f8-9b61-414e-8885-3033b67c2850","metadata":{"id":"e68ea9f8-9b61-414e-8885-3033b67c2850"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pip in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (24.0)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (2.19.0)\n","Requirement already satisfied: transformers in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (4.40.1)\n","Requirement already satisfied: accelerate in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (0.29.3)\n","Requirement already satisfied: soundfile in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (0.12.1)\n","Requirement already satisfied: librosa in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (0.10.1)\n","Requirement already satisfied: evaluate in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (0.4.2)\n","Requirement already satisfied: jiwer in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (3.0.3)\n","Requirement already satisfied: tensorboard in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (2.16.2)\n","Requirement already satisfied: gradio in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (4.28.3)\n","Requirement already satisfied: filelock in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from datasets) (1.24.4)\n","Requirement already satisfied: pyarrow>=12.0.0 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from datasets) (15.0.0)\n","Requirement already satisfied: pyarrow-hotfix in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from datasets) (2.2.1)\n","Requirement already satisfied: requests>=2.19.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from datasets) (4.66.2)\n","Requirement already satisfied: xxhash in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.2.0)\n","Requirement already satisfied: aiohttp in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from datasets) (3.9.3)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from datasets) (0.22.2)\n","Requirement already satisfied: packaging in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from datasets) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from transformers) (0.4.2)\n","Requirement already satisfied: psutil in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from accelerate) (5.9.8)\n","Requirement already satisfied: torch>=1.10.0 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from accelerate) (2.2.1)\n","Requirement already satisfied: cffi>=1.0 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from soundfile) (1.16.0)\n","Requirement already satisfied: audioread>=2.1.9 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from librosa) (3.0.1)\n","Requirement already satisfied: scipy>=1.2.0 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from librosa) (1.10.1)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from librosa) (1.4.1.post1)\n","Requirement already satisfied: joblib>=0.14 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from librosa) (1.3.2)\n","Requirement already satisfied: decorator>=4.3.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from librosa) (5.1.1)\n","Requirement already satisfied: numba>=0.51.0 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from librosa) (0.59.0)\n","Requirement already satisfied: pooch>=1.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from librosa) (1.8.1)\n","Requirement already satisfied: soxr>=0.3.2 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from librosa) (0.3.7)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from librosa) (4.10.0)\n","Requirement already satisfied: lazy-loader>=0.1 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from librosa) (0.3)\n","Requirement already satisfied: msgpack>=1.0 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from librosa) (1.0.8)\n","Requirement already satisfied: click<9.0.0,>=8.1.3 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from jiwer) (8.1.7)\n","Requirement already satisfied: rapidfuzz<4,>=3 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from jiwer) (3.6.1)\n","Requirement already satisfied: absl-py>=0.4 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from tensorboard) (2.1.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from tensorboard) (1.62.0)\n","Requirement already satisfied: markdown>=2.6.8 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from tensorboard) (3.5.2)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from tensorboard) (4.25.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from tensorboard) (69.1.0)\n","Requirement already satisfied: six>1.9 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from tensorboard) (3.0.1)\n","Requirement already satisfied: aiofiles<24.0,>=22.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from gradio) (23.2.1)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from gradio) (5.2.0)\n","Requirement already satisfied: fastapi in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from gradio) (0.110.0)\n","Requirement already satisfied: ffmpy in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from gradio) (0.3.2)\n","Requirement already satisfied: gradio-client==0.16.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from gradio) (0.16.0)\n","Requirement already satisfied: httpx>=0.24.1 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from gradio) (0.27.0)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from gradio) (6.1.2)\n","Requirement already satisfied: jinja2<4.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from gradio) (3.1.3)\n","Requirement already satisfied: markupsafe~=2.0 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from gradio) (2.1.5)\n","Requirement already satisfied: matplotlib~=3.0 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from gradio) (3.8.3)\n","Requirement already satisfied: orjson~=3.0 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from gradio) (3.9.15)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from gradio) (10.2.0)\n","Requirement already satisfied: pydantic>=2.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from gradio) (2.6.3)\n","Requirement already satisfied: pydub in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from gradio) (0.25.1)\n","Requirement already satisfied: python-multipart>=0.0.9 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from gradio) (0.0.9)\n","Requirement already satisfied: ruff>=0.2.2 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from gradio) (0.3.0)\n","Requirement already satisfied: semantic-version~=2.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from gradio) (2.10.0)\n","Requirement already satisfied: tomlkit==0.12.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from gradio) (0.12.0)\n","Requirement already satisfied: typer<1.0,>=0.12 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from gradio) (0.12.3)\n","Requirement already satisfied: urllib3~=2.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from gradio) (2.2.1)\n","Requirement already satisfied: uvicorn>=0.14.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from gradio) (0.27.1)\n","Requirement already satisfied: websockets<12.0,>=10.0 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from gradio-client==0.16.0->gradio) (11.0.3)\n","Requirement already satisfied: jsonschema>=3.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from altair<6.0,>=4.2.0->gradio) (4.21.1)\n","Requirement already satisfied: toolz in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n","Requirement already satisfied: pycparser in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from cffi>=1.0->soundfile) (2.21)\n","Requirement already satisfied: aiosignal>=1.1.2 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: anyio in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (4.3.0)\n","Requirement already satisfied: certifi in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n","Requirement already satisfied: httpcore==1.* in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (1.0.4)\n","Requirement already satisfied: idna in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (3.6)\n","Requirement already satisfied: sniffio in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from matplotlib~=3.0->gradio) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from matplotlib~=3.0->gradio) (4.49.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n","Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from numba>=0.51.0->librosa) (0.42.0)\n","Requirement already satisfied: pytz>=2020.1 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: platformdirs>=2.5.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from pooch>=1.0->librosa) (4.2.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from pydantic>=2.0->gradio) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from pydantic>=2.0->gradio) (2.16.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from scikit-learn>=0.20.0->librosa) (3.3.0)\n","Requirement already satisfied: sympy in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n","Requirement already satisfied: shellingham>=1.3.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n","Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from fastapi->gradio) (0.36.3)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.33.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /home/lvasina/.virtualenvs/bp/lib64/python3.11/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.17.2)\n","Requirement already satisfied: mpmath>=0.19 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"]}],"source":["!pip install --upgrade pip\n","!pip install --upgrade datasets transformers accelerate soundfile librosa evaluate jiwer tensorboard gradio"]},{"cell_type":"markdown","id":"4cd77d77","metadata":{},"source":["## To continue create huggingface profile and create a access token\n","Pass it to the following code cell to login to huggingface."]},{"cell_type":"code","execution_count":5,"id":"b045a39e-2a3e-4153-bdb5-281500bcd348","metadata":{"id":"b045a39e-2a3e-4153-bdb5-281500bcd348"},"outputs":[],"source":["import yaml\n","with open('../../data/SECRETS.yaml', 'r') as file:\n","    secrets = yaml.safe_load(file)"]},{"cell_type":"code","execution_count":6,"id":"63c7da1c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /home/lvasina/.cache/huggingface/token\n","Login successful\n"]}],"source":["from huggingface_hub import login\n","login(secrets['huggingface_write_token'])"]},{"cell_type":"markdown","id":"b219c9dd-39b6-4a95-b2a1-3f547a1e7bc0","metadata":{"id":"b219c9dd-39b6-4a95-b2a1-3f547a1e7bc0"},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":4,"id":"a2787582-554f-44ce-9f38-4180a5ed6b44","metadata":{"id":"a2787582-554f-44ce-9f38-4180a5ed6b44"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages/datasets/load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n","You can remove this warning by passing 'token=<use_auth_token>' instead.\n","  warnings.warn(\n","/home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages/datasets/load.py:1486: FutureWarning: The repository for mozilla-foundation/common_voice_11_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_11_0\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n","  warnings.warn(\n","/home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages/datasets/load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n","You can remove this warning by passing 'token=<use_auth_token>' instead.\n","  warnings.warn(\n","/home/lvasina/.virtualenvs/bp/lib/python3.11/site-packages/datasets/load.py:1486: FutureWarning: The repository for mozilla-foundation/common_voice_11_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_11_0\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n","        num_rows: 22155\n","    })\n","    test: Dataset({\n","        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n","        num_rows: 7714\n","    })\n","})\n"]}],"source":["from datasets import load_dataset, DatasetDict, concatenate_datasets\n","\n","common_voice = DatasetDict()\n","\n","common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"cs\", split=\"train+validation\", use_auth_token=True)\n","common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"cs\", split=\"test\", use_auth_token=True)\n","\n","print(common_voice)"]},{"cell_type":"code","execution_count":5,"id":"20ba635d-518c-47ac-97ee-3cad25f1e0ce","metadata":{"id":"20ba635d-518c-47ac-97ee-3cad25f1e0ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 22155\n","    })\n","    test: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 7714\n","    })\n","})\n"]}],"source":["common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n","print(common_voice)\n"]},{"cell_type":"markdown","id":"2d63b2d2-f68a-4d74-b7f1-5127f6d16605","metadata":{"id":"2d63b2d2-f68a-4d74-b7f1-5127f6d16605"},"source":["## Prepare Feature Extractor, Tokenizer and Data"]},{"cell_type":"markdown","id":"560332eb-3558-41a1-b500-e83a9f695f84","metadata":{"id":"560332eb-3558-41a1-b500-e83a9f695f84"},"source":["### Load WhisperFeatureExtractor"]},{"cell_type":"code","execution_count":6,"id":"bc77d7bb-f9e2-47f5-b663-30f7a4321ce5","metadata":{"id":"bc77d7bb-f9e2-47f5-b663-30f7a4321ce5"},"outputs":[],"source":["from transformers import WhisperFeatureExtractor\n","\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")"]},{"cell_type":"markdown","id":"93748af7-b917-4ecf-a0c8-7d89077ff9cb","metadata":{"id":"93748af7-b917-4ecf-a0c8-7d89077ff9cb"},"source":["### Load WhisperTokenizer"]},{"cell_type":"code","execution_count":7,"id":"c7b07f9b-ae0e-4f89-98f0-0c50d432eab6","metadata":{"colab":{"referenced_widgets":["90d056e20b3e4f14ae0199a1a4ab1bb0","d82a88daec0e4f14add691b7b903064c","350acdb0f40e454099fa901e66de55f0","2e6a82a462cc411d90fa1bea4ee60790","c74bfee0198b4817832ea86e8e88d96c","04fb2d81eff646068e10475a08ae42f4"]},"id":"c7b07f9b-ae0e-4f89-98f0-0c50d432eab6","outputId":"5c004b44-86e7-4e00-88be-39e0af5eed69"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["from transformers import WhisperTokenizer\n","\n","tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=\"czech\", task=\"transcribe\")"]},{"cell_type":"markdown","id":"d2ef23f3-f4a8-483a-a2dc-080a7496cb1b","metadata":{"id":"d2ef23f3-f4a8-483a-a2dc-080a7496cb1b"},"source":["### Combine To Create A WhisperProcessor"]},{"cell_type":"code","execution_count":8,"id":"77d9f0c5-8607-4642-a8ac-c3ab2e223ea6","metadata":{"id":"77d9f0c5-8607-4642-a8ac-c3ab2e223ea6"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["from transformers import WhisperProcessor\n","\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"czech\", task=\"transcribe\")"]},{"cell_type":"markdown","id":"381acd09-0b0f-4d04-9eb3-f028ac0e5f2c","metadata":{"id":"381acd09-0b0f-4d04-9eb3-f028ac0e5f2c"},"source":["### Prepare Data"]},{"cell_type":"code","execution_count":9,"id":"6e6b0ec5-0c94-4e2c-ae24-c791be1b2255","metadata":{"id":"6e6b0ec5-0c94-4e2c-ae24-c791be1b2255"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'audio': {'path': '/home/lvasina/.cache/huggingface/datasets/downloads/extracted/981d0d71b7ccdd8440b5a30d1846b0edbf80c7c3d4d6941b7d3193319a582fb6/cs_train_0/common_voice_cs_25695144.mp3', 'array': array([ 4.26325641e-14,  1.13686838e-13,  2.62900812e-13, ...,\n","       -1.01048208e-04, -1.48227118e-04, -8.67909548e-05]), 'sampling_rate': 48000}, 'sentence': 'S judem začínala v rodném Kjóto.'}\n"]}],"source":["print(common_voice[\"train\"][0])"]},{"cell_type":"code","execution_count":10,"id":"f12e2e57-156f-417b-8cfb-69221cc198e8","metadata":{"id":"f12e2e57-156f-417b-8cfb-69221cc198e8"},"outputs":[],"source":["from datasets import Audio\n","\n","common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"]},{"cell_type":"code","execution_count":11,"id":"87122d71-289a-466a-afcf-fa354b18946b","metadata":{"id":"87122d71-289a-466a-afcf-fa354b18946b"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'audio': {'path': '/home/lvasina/.cache/huggingface/datasets/downloads/extracted/981d0d71b7ccdd8440b5a30d1846b0edbf80c7c3d4d6941b7d3193319a582fb6/cs_train_0/common_voice_cs_25695144.mp3', 'array': array([ 8.73114914e-11,  1.74622983e-10, -5.82076609e-11, ...,\n","       -2.33804210e-04, -5.76644670e-05, -5.10758255e-05]), 'sampling_rate': 16000}, 'sentence': 'S judem začínala v rodném Kjóto.'}\n"]}],"source":["print(common_voice[\"train\"][0])"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# from audiomentations import (\n","#     AddBackgroundNoise,\n","#     AddGaussianNoise,\n","#     Compose,Gain,OneOf,LowShelfFilter,PitchShift,PolarityInversion,TimeStretch,Mp3Compression,PeakingFilter,SevenBandParametricEQ\n","# )\n","\n","# # define augmentation\n","# augmentation = Compose(\n","#     [\n","#         # TimeStretch(min_rate=0.9, max_rate=1.1, p=0.2, leave_length_unchanged=False),\n","#         # Gain(min_gain_in_db=-6, max_gain_in_db=6, p=0.1),\n","#         # PitchShift(min_semitones=-4, max_semitones=4, p=0.2),\n","#     #AddGaussianNoise(min_amplitude=0.005, max_amplitude=0.015, p=1.0),\n","#     LowShelfFilter(min_center_freq=20, max_center_freq=600, min_gain_db=-16.0, max_gain_db=16.0, min_q=0.5, max_q=1.0, p=1),\n","#     Mp3Compression(min_bitrate=8, max_bitrate=64, backend='pydub', p=1),\n","#     PeakingFilter(min_center_freq=51, max_center_freq=7400, min_gain_db=-22, max_gain_db=22, min_q=0.5, max_q=1.0, p=1),\n","#     SevenBandParametricEQ(min_gain_db=-10, max_gain_db=10, p=1),\n","#     ]\n","# )\n","\n","# def augment_dataset(batch):\n","#     # load and (possibly) resample audio data to 16kHz\n","#     sample = batch['audio']\n","\n","#     # apply augmentation\n","#     #print(f\"SAMPLE ENTERING AUGMENTATION FUNCTION: {sample['array']}\")\n","#     augmented_waveform = augmentation(sample['array'], sample_rate=sample['sampling_rate'])\n","#     batch['audio']['array'] = augmented_waveform\n","#     return batch\n","\n","# common_voice = common_voice.map(augment_dataset, num_proc=1)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["SOX FILE ['--sox=\"norm gain 20 highpass 300 phaser 0.5 0.6 1 0.45 0.6 -s\"\\n', '--sox=\"norm gain 20 highpass 300 phaser 0.5 0.6 1 0.45 0.6 -s\" mp3 bitrate 8\\n', '--sox=\"norm gain 20 highpass 300 phaser 0.5 0.6 1 0.45 0.6 -s\" pcm_mulaw\\n', '--sox=\"norm gain 20 highpass 300 phaser 0.5 0.6 1 0.45 0.6 -s\" g726 audio_bitrate 40k\\n', '--sox=\"norm gain 20 highpass 300 phaser 0.5 0.6 1 0.45 0.6 -s\" gsm\\n', '--sox=\"norm gain 20 highpass 300 phaser 0.5 0.6 1 0.45 0.6 -s\" amr audio_bitrate 4.75k'] <class 'list'>\n","ADDED: TimeStretch, \n","\t\t{'min_rate': 0.9, 'max_rate': 1.1, 'p': 0.2, 'leave_length_unchanged': False}\n","\n","ADDED: Gain, \n","\t\t{'min_gain_in_db': -6, 'max_gain_in_db': 6, 'p': 0.1}\n","\n","ADDED: PitchShift, \n","\t\t{'sample_rate': 16000, 'n_steps': [1, 1.5, 0.1]}\n","\n","ADDED: AddGaussianNoise, \n","\t\t{'min_amplitude': 0.005, 'max_amplitude': 0.015, 'p': 0.2}\n","\n","ADDED: LowPassFilter, \n","\t\t{'min_cutoff_freq': 700, 'max_cutoff_freq': 800, 'sample_rate': 16000, 'p': 0.1}\n","\n","ADDED: Mp3Compression, \n","\t\t{'min_bitrate': 8, 'max_bitrate': 8, 'backend': 'pydub', 'p': 0.2}\n","\n","\n","\n","FINAL TRANSFORMATIONS LIST:\n","<audiomentations.augmentations.time_stretch.TimeStretch object at 0x7f63dd4e6050>\n","<audiomentations.augmentations.gain.Gain object at 0x7f63dd29b5d0>\n","{'pitchshift': {'sample_rate': 16000, 'n_steps': [1, 1.5, 0.1]}, 'p': 0.2}\n","<audiomentations.augmentations.add_gaussian_noise.AddGaussianNoise object at 0x7f63dd29ab10>\n","LowPassFilter()\n","<audiomentations.augmentations.mp3_compression.Mp3Compression object at 0x7f63dd843d10>\n","[<audiomentations.augmentations.time_stretch.TimeStretch object at 0x7f63dd4e6050>, <audiomentations.augmentations.gain.Gain object at 0x7f63dd29b5d0>, {'pitchshift': {'sample_rate': 16000, 'n_steps': [1, 1.5, 0.1]}, 'p': 0.2}, <audiomentations.augmentations.add_gaussian_noise.AddGaussianNoise object at 0x7f63dd29ab10>, LowPassFilter(), <audiomentations.augmentations.mp3_compression.Mp3Compression object at 0x7f63dd843d10>]\n"]}],"source":["import sys\n","sys.path.append('../../AudioAugmentor')\n","from tqdm import tqdm_notebook as tqdm\n","import sox_parser\n","import importlib\n","import core\n","import transf_gen\n","import torch.multiprocessing as mp\n","import IPython\n","from scipy.signal import fftconvolve\n","from scipy.io import wavfile\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pyroomacoustics as pra\n","import soundfile as sf\n","import audiomentations as AA\n","import torch_audiomentations as TA\n","import ffmpeg\n","from rir_setup import ApplyRIR, get_all_materials_info\n","import rir_setup\n","import torch\n","import torchaudio\n","import torchaudio.transforms as T\n","import os\n","from IPython.display import Audio, display\n","import io\n","from pydub import AudioSegment\n","import sys\n","import random\n","importlib.reload(core)\n","importlib.reload(transf_gen)\n","importlib.reload(sox_parser)\n","\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","random.seed(0)\n","np.random.seed(0)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","sampling_rate = 16000\n","\n","example_sox = '--sox=\"norm gain 20 highpass 300 phaser 0.5 0.6 1 0.45 0.6 -s\"'\n","#example_sox = '--sox=\"norm gain 20 highpass 300 phaser 0.5 0.6 1 0.45 0.6 -s\" mp3 bitrate 8'\n","#example_sox = '--sox=\"norm gain 20 highpass 300 phaser 0.5 0.6 1 0.45 0.6 -s\" pcm_mulaw'\n","#example_sox = '--sox=\"norm gain 20 highpass 300 phaser 0.5 0.6 1 0.45 0.6 -s\" g726 audio_bitrate 40k'\n","#example_sox = '--sox=\"norm gain 20 highpass 300 phaser 0.5 0.6 1 0.45 0.6 -s\" gsm'\n","example_sox = '--sox=\"norm gain 20 highpass 300 phaser 0.5 0.6 1 0.45 0.6 -s\" amr audio_bitrate 4.75k'\n","\n","sox_file_content = sox_parser.load_sox_file('../../data/sox_file_example.txt')\n","print('SOX FILE', sox_file_content, type(sox_file_content))\n","\n","rir_kwargs = {\n","    'audio_sample_rate': sampling_rate,\n","    'corners_coord': [[0, 0], [0, 3], [5, 3], [5, 1], [3, 1], [3, 0]],\n","    'walls_mat': 'curtains_cotton_0.5',\n","    'room_height': 2.0,\n","    'max_order': 3,\n","    'floor_mat': 'carpet_cotton',\n","    'ceiling_mat': 'hard_surface',\n","    'ray_tracing': True,\n","    'air_absorption': True,\n","    'source_coord': [[1.0], [1.0], [0.5]],\n","    'microphones_coord': [[3.5], [2.0], [0.5]],\n","}\n","\n","rir_kwargs = {\n","    'audio_sample_rate': sampling_rate,\n","    'x_range': (0, 10), \n","    'y_range': (0, 10), \n","    'num_vertices_range': (4, 4),\n","    'mic_height': 1.5,\n","    'source_height': 1.5,\n","    'walls_mat': 'curtains_cotton_0.5',\n","    'room_height': 2.0,\n","    'max_order': 3,\n","    'floor_mat': 'carpet_cotton',\n","    'ceiling_mat': 'hard_surface',\n","    'ray_tracing': True,\n","    'air_absorption': True,\n","}\n","\n","transformations = [\n","    # core.torch_randomizer(T.Vol(2), 0.1),\n","    # core.torch_randomizer(T.Speed(orig_freq=sampling_rate, factor=1.3), 0.1),\n","    # ApplyRIR(**rir_kwargs),\n","    # T.Vol(1.5),\n","    # T.PitchShift(sample_rate=16000, n_steps=1.5),\n","    # T.Speed(orig_freq=sampling_rate, factor=0.7),\n","    # AA.TimeStretch(min_rate=0.9, max_rate=1.1, p=0.2,\n","    #                leave_length_unchanged=False),\n","    # AA.Gain(min_gain_in_db=-6, max_gain_in_db=6, p=0.1),\n","    # AA.PitchShift(min_semitones=-4, max_semitones=4, p=0.2),\n","    # AA.AddGaussianNoise(min_amplitude=0.005, max_amplitude=0.015, p=0.2),\n","    # TA.LowPassFilter(min_cutoff_freq=700, max_cutoff_freq=800,\n","    #                  sample_rate=sampling_rate, p=0.1),\n","    # TA.AddBackgroundNoise(background_paths='/home/lvasina/Desktop/IBP/BP/data/musan/noise/free-sound', min_snr_in_db=10, max_snr_in_db=20, p=0.3, sample_rate=sampling_rate),\n","    # AA.LowShelfFilter(min_center_freq=20, max_center_freq=600, min_gain_db=-16.0, max_gain_db=16.0, min_q=0.5, max_q=1.0, p=1),\n","    # AA.Mp3Compression(min_bitrate=8, max_bitrate=64, backend='pydub', p=1),\n","    # AA.PeakingFilter(min_center_freq=51, max_center_freq=7400, min_gain_db=-22, max_gain_db=22, min_q=0.5, max_q=1.0, p=1),\n","    # AA.SevenBandParametricEQ(min_gain_db=-10, max_gain_db=10, p=1),\n","]\n","\n","transformations = transf_gen.transf_gen(verbose=True,\n","                                        # ApplyRIR=rir_kwargs,\n","                                        # Vol={'gain': [0.9, 1.5, 0.1],\n","                                        #      'p': 1.0},\n","                                        # Speed={'orig_freq': sampling_rate,\n","                                        #        'factor': [0.5, 1.5, 0.1],\n","                                        #        'p': 0.1},\n","\n","                                        TimeStretch='min_rate=0.9, max_rate=1.1, p=0.2, leave_length_unchanged=False',\n","                                        Gain='min_gain_in_db=-6, max_gain_in_db=6, p=0.1',\n","                                        PitchShift={'sample_rate': sampling_rate,\n","                                                     'n_steps': [1, 1.5, 0.1],\n","                                                     'p': 0.2},\n","                                        AddGaussianNoise='min_amplitude=0.005, max_amplitude=0.015, p=0.2',          \n","                                        LowPassFilter={\n","                                            'min_cutoff_freq': 700,\n","                                            'max_cutoff_freq': 800,\n","                                            'sample_rate': sampling_rate,\n","                                            'p': 0.1},\n","                                        Mp3Compression={'min_bitrate': 8,\n","                                                        'max_bitrate': 8,\n","                                                        'backend': 'pydub',\n","                                                        'p': 0.2},\n","                                                        \n","                                        # pcm_alaw=True,\n","                                        # gsm=True,\n","                                        # g726={'audio_bitrate': '16k'},\n","                                        # amr={'audio_bitrate': '4.75k'},\n","                                        )\n","print(transformations)\n","\n","with open('../../data/sox_file_example.txt', 'r') as file:\n","    sox_file_content = file.readlines()\n","\n","# Augment using AudioAugmentor\n","def augment_dataset(batch):\n","    # load and (possibly) resample audio data to 16kHz\n","    sample = batch['audio']\n","    augment = core.AugmentWaveform(\n","        transformations=transformations, device='cpu', sox_effects=None, sample_rate=sample['sampling_rate'], verbose=False\n","        #transformations=transformations, device='cuda', sox_effects=None, sample_rate=sample['sampling_rate'], verbose=False\n","        #transformations=None, device='cpu', sox_effects=example_sox, sample_rate=sample['sampling_rate'], verbose=True\n","        #transformations=None, device='cpu', sox_effects=sox_file_content, sample_rate=sample['sampling_rate'], verbose=True\n","    )\n","    # apply augmentation\n","    # print(f\"SAMPLE ENTERING AUGMENTATION FUNCTION: {sample['array']}\")\n","    augmented_waveform = augment(sample['array'])\n","    batch['audio']['array'] = augmented_waveform\n","    return batch\n","\n","# Augment using Audiomentaions\n","# def augment_dataset(batch):\n","#     # load and (possibly) resample audio data to 16kHz\n","#     sample = batch['audio']\n","#     augment = AA.Compose([\n","#         AA.AddGaussianNoise(min_amplitude=0.005, max_amplitude=0.015, p=1)\n","# ])\n","#     # apply augmentation\n","#     # print(f\"SAMPLE ENTERING AUGMENTATION FUNCTION: {sample['array']}\")\n","#     augmented_waveform = augment(sample['array'], sample['sampling_rate'])\n","#     batch['audio']['array'] = augmented_waveform\n","#     return batch\n","\n","\n","# from datasets.utils.logging import set_verbosity_debug\n","# set_verbosity_debug()\n","\n","# common_voice['train'] = common_voice['train'].map(augment_dataset, num_proc=os.cpu_count())\n"]},{"cell_type":"code","execution_count":14,"id":"5126c979","metadata":{},"outputs":[],"source":["CURRENT_RUN_NAME = \"whisper-base-cs-cv11-timestetch02-gain01-pitch02-gaussian02-lowpass01-timemask50-freqmask50\""]},{"cell_type":"code","execution_count":15,"id":"0412a62c","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad1014300d0d4737923eb702fc1a29ae","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=8):   0%|          | 0/22155 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"712b4315a792452088a042b27ac9b216","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=8):   0%|          | 0/7714 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["SAVED PICKLE FILE\n"]}],"source":["if os.path.exists(f'{CURRENT_RUN_NAME}-TEST-BEFORE_FEATURE_EXTR.pkl'):\n","    # Open the file in binary mode \n","    with open(f'{CURRENT_RUN_NAME}-TEST-BEFORE_FEATURE_EXTR.pkl', 'rb') as file: \n","        \n","        # Call load method to deserialze \n","        augmented_train_set = pickle.load(file) \n","    \n","        print(f'LOADED: \\n{augmented_train_set}')\n","else:\n","    #augmented_set = common_voice['test'].map(augment_dataset, num_proc=os.cpu_count(), writer_batch_size=500)\n","    #augmented_set = common_voice['train'].map(augment_dataset, num_proc=os.cpu_count(), writer_batch_size=500)\n","    augmented_set = common_voice.map(augment_dataset, num_proc=os.cpu_count(), writer_batch_size=500)\n","\n","    # Open a file and use dump()\n","    with open(f'{CURRENT_RUN_NAME}-TEST-BEFORE_FEATURE_EXTR.pkl', 'wb') as file: \n","        print('SAVED PICKLE FILE')\n","        # A new file will be created \n","        pickle.dump(augmented_set, file)"]},{"cell_type":"code","execution_count":16,"id":"6525c478-8962-4394-a1c4-103c54cce170","metadata":{"id":"6525c478-8962-4394-a1c4-103c54cce170"},"outputs":[],"source":["def prepare_dataset(batch):\n","    audio = batch[\"audio\"]\n","\n","    # compute log-Mel input features from input audio array\n","    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n","\n","    # encode target text to label ids\n","    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n","\n","    return batch"]},{"cell_type":"code","execution_count":17,"id":"3cc2346e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 22155\n","    })\n","    test: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 7714\n","    })\n","})\n"]}],"source":["print(common_voice)"]},{"cell_type":"code","execution_count":18,"id":"3ed35780","metadata":{},"outputs":[],"source":["# common_voice['train'] = concatenate_datasets([common_voice['train'], augmented_set])\n","# common_voice['test'] =  augmented_set\n","# common_voice['train'] =  augmented_set\n","common_voice =  augmented_set"]},{"cell_type":"code","execution_count":19,"id":"f1a4e616","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 22155\n","    })\n","    test: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 7714\n","    })\n","})\n"]}],"source":["print(common_voice)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"51c27ebadd254baca464246e51215bfe","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=8):   0%|          | 0/22155 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e6658a1222a45759b4128b3214a7b77","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=8):   0%|          | 0/7714 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["SAVED PICKLE FILE\n"]}],"source":["if os.path.exists(f'{CURRENT_RUN_NAME}-READY.pkl'):\n","    # Open the file in binary mode \n","    with open(f'{CURRENT_RUN_NAME}-READY.pkl', 'rb') as file: \n","        \n","        # Call load method to deserialze \n","        common_voice = pickle.load(file) \n","    \n","        print(f'LOADED: \\n{common_voice}') \n","else:\n","    common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=os.cpu_count(), writer_batch_size=500)\n","    # Open a file and use dump() \n","    with open(f'{CURRENT_RUN_NAME}-READY.pkl', 'wb') as file: \n","        print('SAVED PICKLE FILE')\n","        # A new file will be created \n","        pickle.dump(common_voice, file)"]},{"cell_type":"markdown","id":"263a5a58-0239-4a25-b0df-c625fc9c5810","metadata":{"id":"263a5a58-0239-4a25-b0df-c625fc9c5810"},"source":["## Training and Evaluation"]},{"cell_type":"markdown","id":"8d230e6d-624c-400a-bbf5-fa660881df25","metadata":{"id":"8d230e6d-624c-400a-bbf5-fa660881df25"},"source":["### Define a Data Collator"]},{"cell_type":"code","execution_count":21,"id":"8326221e-ec13-4731-bb4e-51e5fc1486c5","metadata":{"id":"8326221e-ec13-4731-bb4e-51e5fc1486c5"},"outputs":[],"source":["import torch\n","\n","from dataclasses import dataclass\n","from typing import Any, Dict, List, Union\n","\n","@dataclass\n","class DataCollatorSpeechSeq2SeqWithPadding:\n","    processor: Any\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        # split inputs and labels since they have to be of different lengths and need different padding methods\n","        # first treat the audio inputs by simply returning torch tensors\n","        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n","        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n","\n","        # get the tokenized label sequences\n","        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n","        # pad the labels to max length\n","        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n","\n","        # replace padding with -100 to ignore loss correctly\n","        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","\n","        # if bos token is appended in previous tokenization step,\n","        # cut bos token here as it's append later anyways\n","        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n","            labels = labels[:, 1:]\n","\n","        batch[\"labels\"] = labels\n","\n","        return batch\n","    \n","data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"]},{"cell_type":"markdown","id":"d62bb2ab-750a-45e7-82e9-61d6f4805698","metadata":{"id":"d62bb2ab-750a-45e7-82e9-61d6f4805698"},"source":["### Evaluation Metrics"]},{"cell_type":"code","execution_count":23,"id":"23959a70-22d0-4ffe-9fa1-72b61e75bb52","metadata":{"id":"23959a70-22d0-4ffe-9fa1-72b61e75bb52"},"outputs":[],"source":["import evaluate\n","\n","metric = evaluate.load(\"wer\")\n","\n","def compute_metrics(pred):\n","    pred_ids = pred.predictions\n","    label_ids = pred.label_ids\n","\n","    # replace -100 with the pad_token_id\n","    label_ids[label_ids == -100] = tokenizer.pad_token_id\n","\n","    # we do not want to group tokens when computing the metrics\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n","\n","    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n","\n","    return {\"wer\": wer}"]},{"cell_type":"markdown","id":"daf2a825-6d9f-4a23-b145-c37c0039075b","metadata":{"id":"daf2a825-6d9f-4a23-b145-c37c0039075b"},"source":["## Load a Pre-Trained Checkpoint"]},{"cell_type":"code","execution_count":24,"id":"5a10cc4b-07ec-4ebd-ac1d-7c601023594f","metadata":{"id":"5a10cc4b-07ec-4ebd-ac1d-7c601023594f"},"outputs":[],"source":["from transformers import WhisperForConditionalGeneration\n","\n","model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")"]},{"cell_type":"code","execution_count":25,"id":"dc61f451","metadata":{},"outputs":[],"source":["# THIS WAS ADDED BEACUSE OF THE ISSUE STATED HERE: https://github.com/huggingface/blog/issues/1794 \n","# AND HERE: https://github.com/huggingface/transformers/issues/28814\n","model.generation_config.language = \"cs\" "]},{"cell_type":"code","execution_count":26,"id":"62038ba3-88ed-4fce-84db-338f50dcd04f","metadata":{"id":"62038ba3-88ed-4fce-84db-338f50dcd04f"},"outputs":[],"source":["model.config.forced_decoder_ids = None\n","model.config.suppress_tokens = []"]},{"cell_type":"code","execution_count":27,"id":"ae490eb7","metadata":{},"outputs":[],"source":["# model.config.apply_spec_augment = True\n","# model.config.mask_time_prob = 0.20\n","# model.config.mask_feature_prob = 0.20"]},{"cell_type":"markdown","id":"2178dea4-80ca-47b6-b6ea-ba1915c90c06","metadata":{"id":"2178dea4-80ca-47b6-b6ea-ba1915c90c06"},"source":["### Define the Training Configuration"]},{"cell_type":"code","execution_count":28,"id":"0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a","metadata":{"id":"0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a"},"outputs":[],"source":["from transformers import Seq2SeqTrainingArguments\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=f'./{CURRENT_RUN_NAME}',  # change to a repo name\n","    per_device_train_batch_size=8,\n","    gradient_accumulation_steps=4,  # increase by 2x for every 2x decrease in batch size\n","    learning_rate=1e-5,\n","    warmup_steps=500,\n","    max_steps=4000,\n","    gradient_checkpointing=True,\n","    fp16=True,\n","    evaluation_strategy=\"steps\",\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    generation_max_length=225,\n","    save_steps=1000,\n","    eval_steps=1000,\n","    logging_steps=25,\n","    report_to=[\"tensorboard\"],\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"wer\",\n","    greater_is_better=False,\n","    push_to_hub=True,\n",")"]},{"cell_type":"code","execution_count":29,"id":"d546d7fe-0543-479a-b708-2ebabec19493","metadata":{"id":"d546d7fe-0543-479a-b708-2ebabec19493"},"outputs":[{"name":"stderr","output_type":"stream","text":["max_steps is given, it will override any value given in num_train_epochs\n"]},{"data":{"text/plain":["[]"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import Seq2SeqTrainer\n","\n","trainer = Seq2SeqTrainer(\n","    args=training_args,\n","    model=model,\n","    train_dataset=common_voice[\"train\"],\n","    eval_dataset=common_voice[\"test\"].shuffle(16),\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=processor.feature_extractor,\n",")\n","\n","# We'll save the processor object once before starting training. Since the processor is not trainable, it won't change over the course of training:\n","processor.save_pretrained(training_args.output_dir)"]},{"cell_type":"markdown","id":"7f404cf9-4345-468c-8196-4bd101d9bd51","metadata":{"id":"7f404cf9-4345-468c-8196-4bd101d9bd51"},"source":["### Training"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d2c36e7464a04878af20bbc4527e22de","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/965 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 3.206214666366577,\n"," 'eval_wer': 112.0436880686632,\n"," 'eval_runtime': 1948.0194,\n"," 'eval_samples_per_second': 3.96,\n"," 'eval_steps_per_second': 0.495}"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate()"]},{"cell_type":"code","execution_count":1,"id":"50a04990","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","TINY-CS baseline CV11 performance:\n","'eval_wer': 100.73488783290972,\n","\n","\n","SMALL-CS baseline performance:\n","'eval_wer': 46.296091649169334,\n","\n"]}],"source":["print(\"\"\"\n","TINY-CS baseline CV11 performance:\n","'eval_wer': 100.73488783290972,\n","\"\"\")\n","\n","print(\"\"\"\n","SMALL-CS baseline performance:\n","'eval_wer': 46.296091649169334,\n","\"\"\")"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","BASE-CS baseline performance:whisper-bs-cs-train-noaug-test-noaug\n","'eval_wer': 72.12583342542455,\n","\n","\n","BASE-CS baseline performance:whisper-bs-cs-train-noaug-test-tstretch20-gain10-pitch20-gaussian20-lowpass10-mp3\n","'eval_wer': 109.89796294249825,\n","\n"]}],"source":["print(\"\"\"\n","BASE-CS baseline performance:whisper-bs-cs-train-noaug-test-noaug\n","'eval_wer': 72.12583342542455,\n","\"\"\")\n","\n","print(\"\"\"\n","BASE-CS baseline performance:whisper-bs-cs-train-noaug-test-tstretch20-gain10-pitch20-gaussian20-lowpass10-mp3\n","'eval_wer': 109.89796294249825,\n","\"\"\")"]},{"cell_type":"code","execution_count":33,"id":"ee8b7b8e-1c9a-4d77-9137-1778a629e6de","metadata":{"id":"ee8b7b8e-1c9a-4d77-9137-1778a629e6de"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8203748425524b27bfd88646ab8ee02c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 3.0544, 'grad_norm': 32.488033294677734, 'learning_rate': 4.800000000000001e-07, 'epoch': 0.04}\n","{'loss': 2.7699, 'grad_norm': 21.935815811157227, 'learning_rate': 9.800000000000001e-07, 'epoch': 0.07}\n","{'loss': 2.2183, 'grad_norm': 17.920761108398438, 'learning_rate': 1.48e-06, 'epoch': 0.11}\n","{'loss': 1.743, 'grad_norm': 11.836263656616211, 'learning_rate': 1.98e-06, 'epoch': 0.14}\n","{'loss': 1.5458, 'grad_norm': 11.166437149047852, 'learning_rate': 2.4800000000000004e-06, 'epoch': 0.18}\n","{'loss': 1.4539, 'grad_norm': 10.780823707580566, 'learning_rate': 2.9800000000000003e-06, 'epoch': 0.22}\n","{'loss': 1.308, 'grad_norm': 9.90709114074707, 'learning_rate': 3.48e-06, 'epoch': 0.25}\n","{'loss': 1.2953, 'grad_norm': 10.09154987335205, 'learning_rate': 3.980000000000001e-06, 'epoch': 0.29}\n","{'loss': 1.2742, 'grad_norm': 9.044551849365234, 'learning_rate': 4.48e-06, 'epoch': 0.32}\n","{'loss': 1.2325, 'grad_norm': 11.049210548400879, 'learning_rate': 4.980000000000001e-06, 'epoch': 0.36}\n","{'loss': 1.1587, 'grad_norm': 9.582121849060059, 'learning_rate': 5.480000000000001e-06, 'epoch': 0.4}\n","{'loss': 1.1754, 'grad_norm': 10.325601577758789, 'learning_rate': 5.98e-06, 'epoch': 0.43}\n","{'loss': 1.1159, 'grad_norm': 9.037779808044434, 'learning_rate': 6.480000000000001e-06, 'epoch': 0.47}\n","{'loss': 1.0598, 'grad_norm': 10.282343864440918, 'learning_rate': 6.98e-06, 'epoch': 0.51}\n","{'loss': 1.039, 'grad_norm': 8.400751113891602, 'learning_rate': 7.48e-06, 'epoch': 0.54}\n","{'loss': 1.0645, 'grad_norm': 8.24945068359375, 'learning_rate': 7.980000000000002e-06, 'epoch': 0.58}\n","{'loss': 1.0289, 'grad_norm': 8.614447593688965, 'learning_rate': 8.48e-06, 'epoch': 0.61}\n","{'loss': 1.0081, 'grad_norm': 9.15835952758789, 'learning_rate': 8.98e-06, 'epoch': 0.65}\n","{'loss': 0.9221, 'grad_norm': 8.829517364501953, 'learning_rate': 9.48e-06, 'epoch': 0.69}\n","{'loss': 0.954, 'grad_norm': 8.375985145568848, 'learning_rate': 9.980000000000001e-06, 'epoch': 0.72}\n","{'loss': 0.9291, 'grad_norm': 6.814940452575684, 'learning_rate': 9.931428571428571e-06, 'epoch': 0.76}\n","{'loss': 0.9047, 'grad_norm': 8.885191917419434, 'learning_rate': 9.86e-06, 'epoch': 0.79}\n","{'loss': 0.8855, 'grad_norm': 8.987168312072754, 'learning_rate': 9.78857142857143e-06, 'epoch': 0.83}\n","{'loss': 0.8745, 'grad_norm': 8.67940902709961, 'learning_rate': 9.717142857142858e-06, 'epoch': 0.87}\n","{'loss': 0.8692, 'grad_norm': 8.891984939575195, 'learning_rate': 9.645714285714286e-06, 'epoch': 0.9}\n","{'loss': 0.819, 'grad_norm': 8.919293403625488, 'learning_rate': 9.574285714285715e-06, 'epoch': 0.94}\n","{'loss': 0.8019, 'grad_norm': 8.824003219604492, 'learning_rate': 9.502857142857144e-06, 'epoch': 0.97}\n","{'loss': 0.8134, 'grad_norm': 7.097334384918213, 'learning_rate': 9.431428571428573e-06, 'epoch': 1.01}\n","{'loss': 0.7059, 'grad_norm': 8.165074348449707, 'learning_rate': 9.360000000000002e-06, 'epoch': 1.05}\n","{'loss': 0.6968, 'grad_norm': 9.64421272277832, 'learning_rate': 9.28857142857143e-06, 'epoch': 1.08}\n","{'loss': 0.6846, 'grad_norm': 7.279764652252197, 'learning_rate': 9.217142857142858e-06, 'epoch': 1.12}\n","{'loss': 0.6794, 'grad_norm': 8.086944580078125, 'learning_rate': 9.145714285714287e-06, 'epoch': 1.16}\n","{'loss': 0.7057, 'grad_norm': 7.711454391479492, 'learning_rate': 9.074285714285716e-06, 'epoch': 1.19}\n","{'loss': 0.7, 'grad_norm': 7.804720878601074, 'learning_rate': 9.002857142857144e-06, 'epoch': 1.23}\n","{'loss': 0.6946, 'grad_norm': 8.886159896850586, 'learning_rate': 8.931428571428573e-06, 'epoch': 1.26}\n","{'loss': 0.6738, 'grad_norm': 8.358294486999512, 'learning_rate': 8.860000000000002e-06, 'epoch': 1.3}\n","{'loss': 0.6545, 'grad_norm': 8.43885612487793, 'learning_rate': 8.788571428571429e-06, 'epoch': 1.34}\n","{'loss': 0.7288, 'grad_norm': 8.628174781799316, 'learning_rate': 8.717142857142858e-06, 'epoch': 1.37}\n","{'loss': 0.6571, 'grad_norm': 9.87529468536377, 'learning_rate': 8.645714285714287e-06, 'epoch': 1.41}\n","{'loss': 0.6476, 'grad_norm': 7.178648471832275, 'learning_rate': 8.574285714285714e-06, 'epoch': 1.44}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23717d9d66a042158627a044da4e27d3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/965 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.8180581331253052, 'eval_wer': 63.13773160938594, 'eval_runtime': 1712.1745, 'eval_samples_per_second': 4.505, 'eval_steps_per_second': 0.564, 'epoch': 1.44}\n","{'loss': 0.6367, 'grad_norm': 7.616935729980469, 'learning_rate': 8.502857142857143e-06, 'epoch': 1.48}\n","{'loss': 0.674, 'grad_norm': 8.686921119689941, 'learning_rate': 8.431428571428572e-06, 'epoch': 1.52}\n","{'loss': 0.6177, 'grad_norm': 9.185637474060059, 'learning_rate': 8.36e-06, 'epoch': 1.55}\n","{'loss': 0.6167, 'grad_norm': 8.570244789123535, 'learning_rate': 8.288571428571429e-06, 'epoch': 1.59}\n","{'loss': 0.6576, 'grad_norm': 9.47664737701416, 'learning_rate': 8.217142857142858e-06, 'epoch': 1.62}\n","{'loss': 0.6607, 'grad_norm': 8.574908256530762, 'learning_rate': 8.145714285714287e-06, 'epoch': 1.66}\n","{'loss': 0.6505, 'grad_norm': 7.943095684051514, 'learning_rate': 8.074285714285714e-06, 'epoch': 1.7}\n","{'loss': 0.5836, 'grad_norm': 7.809476852416992, 'learning_rate': 8.002857142857143e-06, 'epoch': 1.73}\n","{'loss': 0.6468, 'grad_norm': 7.658630847930908, 'learning_rate': 7.931428571428572e-06, 'epoch': 1.77}\n","{'loss': 0.5968, 'grad_norm': 8.121810913085938, 'learning_rate': 7.860000000000001e-06, 'epoch': 1.81}\n","{'loss': 0.6311, 'grad_norm': 7.32362174987793, 'learning_rate': 7.788571428571428e-06, 'epoch': 1.84}\n","{'loss': 0.6251, 'grad_norm': 7.885507106781006, 'learning_rate': 7.717142857142857e-06, 'epoch': 1.88}\n","{'loss': 0.6453, 'grad_norm': 8.123008728027344, 'learning_rate': 7.645714285714286e-06, 'epoch': 1.91}\n","{'loss': 0.5995, 'grad_norm': 7.360328674316406, 'learning_rate': 7.574285714285715e-06, 'epoch': 1.95}\n","{'loss': 0.6233, 'grad_norm': 9.221833229064941, 'learning_rate': 7.502857142857144e-06, 'epoch': 1.99}\n","{'loss': 0.5416, 'grad_norm': 6.346987724304199, 'learning_rate': 7.431428571428572e-06, 'epoch': 2.02}\n","{'loss': 0.4661, 'grad_norm': 7.436275482177734, 'learning_rate': 7.360000000000001e-06, 'epoch': 2.06}\n","{'loss': 0.4836, 'grad_norm': 7.109538555145264, 'learning_rate': 7.28857142857143e-06, 'epoch': 2.09}\n","{'loss': 0.4908, 'grad_norm': 6.5595221519470215, 'learning_rate': 7.217142857142858e-06, 'epoch': 2.13}\n","{'loss': 0.4893, 'grad_norm': 6.317373752593994, 'learning_rate': 7.145714285714286e-06, 'epoch': 2.17}\n","{'loss': 0.4631, 'grad_norm': 5.935085773468018, 'learning_rate': 7.074285714285715e-06, 'epoch': 2.2}\n","{'loss': 0.4806, 'grad_norm': 7.0681867599487305, 'learning_rate': 7.002857142857143e-06, 'epoch': 2.24}\n","{'loss': 0.4928, 'grad_norm': 6.169327259063721, 'learning_rate': 6.931428571428572e-06, 'epoch': 2.27}\n","{'loss': 0.4892, 'grad_norm': 7.617376327514648, 'learning_rate': 6.860000000000001e-06, 'epoch': 2.31}\n","{'loss': 0.4656, 'grad_norm': 6.307580947875977, 'learning_rate': 6.7885714285714286e-06, 'epoch': 2.35}\n","{'loss': 0.4755, 'grad_norm': 7.842961311340332, 'learning_rate': 6.7171428571428576e-06, 'epoch': 2.38}\n","{'loss': 0.4889, 'grad_norm': 7.718071460723877, 'learning_rate': 6.645714285714287e-06, 'epoch': 2.42}\n","{'loss': 0.4628, 'grad_norm': 7.846904277801514, 'learning_rate': 6.574285714285716e-06, 'epoch': 2.45}\n","{'loss': 0.4331, 'grad_norm': 6.083847522735596, 'learning_rate': 6.502857142857143e-06, 'epoch': 2.49}\n","{'loss': 0.4726, 'grad_norm': 7.466146469116211, 'learning_rate': 6.431428571428572e-06, 'epoch': 2.53}\n","{'loss': 0.4677, 'grad_norm': 7.7926025390625, 'learning_rate': 6.360000000000001e-06, 'epoch': 2.56}\n","{'loss': 0.4683, 'grad_norm': 7.037795543670654, 'learning_rate': 6.288571428571429e-06, 'epoch': 2.6}\n","{'loss': 0.457, 'grad_norm': 7.25172233581543, 'learning_rate': 6.220000000000001e-06, 'epoch': 2.64}\n","{'loss': 0.4886, 'grad_norm': 7.51599645614624, 'learning_rate': 6.14857142857143e-06, 'epoch': 2.67}\n","{'loss': 0.4713, 'grad_norm': 6.819488048553467, 'learning_rate': 6.077142857142858e-06, 'epoch': 2.71}\n","{'loss': 0.4555, 'grad_norm': 7.925168991088867, 'learning_rate': 6.005714285714286e-06, 'epoch': 2.74}\n","{'loss': 0.4969, 'grad_norm': 6.750608444213867, 'learning_rate': 5.934285714285715e-06, 'epoch': 2.78}\n","{'loss': 0.4681, 'grad_norm': 7.064089298248291, 'learning_rate': 5.862857142857143e-06, 'epoch': 2.82}\n","{'loss': 0.4879, 'grad_norm': 6.645688056945801, 'learning_rate': 5.791428571428572e-06, 'epoch': 2.85}\n","{'loss': 0.4531, 'grad_norm': 6.960362911224365, 'learning_rate': 5.72e-06, 'epoch': 2.89}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4952a439fd646409d448fd63f131142","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/965 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.7081364989280701, 'eval_wer': 56.870004052013115, 'eval_runtime': 1701.0699, 'eval_samples_per_second': 4.535, 'eval_steps_per_second': 0.567, 'epoch': 2.89}\n","{'loss': 0.4822, 'grad_norm': 7.049036026000977, 'learning_rate': 5.6485714285714285e-06, 'epoch': 2.92}\n","{'loss': 0.5002, 'grad_norm': 6.774135112762451, 'learning_rate': 5.5771428571428575e-06, 'epoch': 2.96}\n","{'loss': 0.4576, 'grad_norm': 7.788026332855225, 'learning_rate': 5.5057142857142865e-06, 'epoch': 3.0}\n","{'loss': 0.3568, 'grad_norm': 6.039063930511475, 'learning_rate': 5.4342857142857155e-06, 'epoch': 3.03}\n","{'loss': 0.3681, 'grad_norm': 6.392737865447998, 'learning_rate': 5.362857142857143e-06, 'epoch': 3.07}\n","{'loss': 0.3709, 'grad_norm': 6.505386829376221, 'learning_rate': 5.291428571428572e-06, 'epoch': 3.1}\n","{'loss': 0.3527, 'grad_norm': 6.562893867492676, 'learning_rate': 5.220000000000001e-06, 'epoch': 3.14}\n","{'loss': 0.3849, 'grad_norm': 6.991857528686523, 'learning_rate': 5.14857142857143e-06, 'epoch': 3.18}\n","{'loss': 0.377, 'grad_norm': 7.984914302825928, 'learning_rate': 5.077142857142857e-06, 'epoch': 3.21}\n","{'loss': 0.3546, 'grad_norm': 6.981137275695801, 'learning_rate': 5.005714285714286e-06, 'epoch': 3.25}\n","{'loss': 0.3934, 'grad_norm': 6.684436321258545, 'learning_rate': 4.934285714285715e-06, 'epoch': 3.29}\n","{'loss': 0.3838, 'grad_norm': 6.940674304962158, 'learning_rate': 4.862857142857143e-06, 'epoch': 3.32}\n","{'loss': 0.3699, 'grad_norm': 6.722662925720215, 'learning_rate': 4.7914285714285715e-06, 'epoch': 3.36}\n","{'loss': 0.3899, 'grad_norm': 6.182572841644287, 'learning_rate': 4.7200000000000005e-06, 'epoch': 3.39}\n","{'loss': 0.3698, 'grad_norm': 5.254348278045654, 'learning_rate': 4.648571428571429e-06, 'epoch': 3.43}\n","{'loss': 0.365, 'grad_norm': 6.778558731079102, 'learning_rate': 4.577142857142858e-06, 'epoch': 3.47}\n","{'loss': 0.3871, 'grad_norm': 6.67544412612915, 'learning_rate': 4.505714285714286e-06, 'epoch': 3.5}\n","{'loss': 0.3699, 'grad_norm': 5.982461929321289, 'learning_rate': 4.434285714285715e-06, 'epoch': 3.54}\n","{'loss': 0.3582, 'grad_norm': 7.0809478759765625, 'learning_rate': 4.362857142857143e-06, 'epoch': 3.57}\n","{'loss': 0.3764, 'grad_norm': 7.241802215576172, 'learning_rate': 4.291428571428572e-06, 'epoch': 3.61}\n","{'loss': 0.3806, 'grad_norm': 6.943745136260986, 'learning_rate': 4.22e-06, 'epoch': 3.65}\n","{'loss': 0.3966, 'grad_norm': 6.350455284118652, 'learning_rate': 4.148571428571429e-06, 'epoch': 3.68}\n","{'loss': 0.3799, 'grad_norm': 6.50092887878418, 'learning_rate': 4.0771428571428574e-06, 'epoch': 3.72}\n","{'loss': 0.3706, 'grad_norm': 6.799655914306641, 'learning_rate': 4.0057142857142864e-06, 'epoch': 3.75}\n","{'loss': 0.3664, 'grad_norm': 6.822283744812012, 'learning_rate': 3.934285714285715e-06, 'epoch': 3.79}\n","{'loss': 0.359, 'grad_norm': 6.1183061599731445, 'learning_rate': 3.862857142857143e-06, 'epoch': 3.83}\n","{'loss': 0.375, 'grad_norm': 6.155055046081543, 'learning_rate': 3.7914285714285722e-06, 'epoch': 3.86}\n","{'loss': 0.3728, 'grad_norm': 5.893911361694336, 'learning_rate': 3.7200000000000004e-06, 'epoch': 3.9}\n","{'loss': 0.3822, 'grad_norm': 7.292677402496338, 'learning_rate': 3.648571428571429e-06, 'epoch': 3.94}\n","{'loss': 0.3617, 'grad_norm': 6.565606117248535, 'learning_rate': 3.5771428571428576e-06, 'epoch': 3.97}\n","{'loss': 0.363, 'grad_norm': 6.739574432373047, 'learning_rate': 3.505714285714286e-06, 'epoch': 4.01}\n","{'loss': 0.2912, 'grad_norm': 4.99462890625, 'learning_rate': 3.4342857142857143e-06, 'epoch': 4.04}\n","{'loss': 0.3293, 'grad_norm': 7.037966728210449, 'learning_rate': 3.3628571428571433e-06, 'epoch': 4.08}\n","{'loss': 0.2973, 'grad_norm': 5.795186996459961, 'learning_rate': 3.2914285714285715e-06, 'epoch': 4.12}\n","{'loss': 0.3216, 'grad_norm': 6.9165873527526855, 'learning_rate': 3.2200000000000005e-06, 'epoch': 4.15}\n","{'loss': 0.3091, 'grad_norm': 5.919669151306152, 'learning_rate': 3.1485714285714287e-06, 'epoch': 4.19}\n","{'loss': 0.3023, 'grad_norm': 6.414547443389893, 'learning_rate': 3.0771428571428573e-06, 'epoch': 4.22}\n","{'loss': 0.2942, 'grad_norm': 7.042233943939209, 'learning_rate': 3.005714285714286e-06, 'epoch': 4.26}\n","{'loss': 0.3122, 'grad_norm': 6.736565589904785, 'learning_rate': 2.9342857142857144e-06, 'epoch': 4.3}\n","{'loss': 0.3048, 'grad_norm': 5.996644496917725, 'learning_rate': 2.8628571428571435e-06, 'epoch': 4.33}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"175d493ef51840e881fa5fb5d82279fb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/965 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.6906446814537048, 'eval_wer': 54.51246914944561, 'eval_runtime': 1637.9738, 'eval_samples_per_second': 4.709, 'eval_steps_per_second': 0.589, 'epoch': 4.33}\n","{'loss': 0.3041, 'grad_norm': 5.907439708709717, 'learning_rate': 2.7914285714285716e-06, 'epoch': 4.37}\n","{'loss': 0.3136, 'grad_norm': 6.546174049377441, 'learning_rate': 2.7200000000000002e-06, 'epoch': 4.4}\n","{'loss': 0.3038, 'grad_norm': 6.906599998474121, 'learning_rate': 2.648571428571429e-06, 'epoch': 4.44}\n","{'loss': 0.3167, 'grad_norm': 5.942595958709717, 'learning_rate': 2.5771428571428574e-06, 'epoch': 4.48}\n","{'loss': 0.3016, 'grad_norm': 6.228283405303955, 'learning_rate': 2.5057142857142856e-06, 'epoch': 4.51}\n","{'loss': 0.3095, 'grad_norm': 6.4183669090271, 'learning_rate': 2.4342857142857146e-06, 'epoch': 4.55}\n","{'loss': 0.3125, 'grad_norm': 6.689169406890869, 'learning_rate': 2.362857142857143e-06, 'epoch': 4.58}\n","{'loss': 0.3083, 'grad_norm': 5.8001017570495605, 'learning_rate': 2.2914285714285718e-06, 'epoch': 4.62}\n","{'loss': 0.3008, 'grad_norm': 5.51475715637207, 'learning_rate': 2.2200000000000003e-06, 'epoch': 4.66}\n","{'loss': 0.3227, 'grad_norm': 6.4888129234313965, 'learning_rate': 2.148571428571429e-06, 'epoch': 4.69}\n","{'loss': 0.3286, 'grad_norm': 7.3289713859558105, 'learning_rate': 2.077142857142857e-06, 'epoch': 4.73}\n","{'loss': 0.2906, 'grad_norm': 5.060085296630859, 'learning_rate': 2.0057142857142857e-06, 'epoch': 4.77}\n","{'loss': 0.3245, 'grad_norm': 5.540537357330322, 'learning_rate': 1.9342857142857143e-06, 'epoch': 4.8}\n","{'loss': 0.3138, 'grad_norm': 6.025337219238281, 'learning_rate': 1.8628571428571429e-06, 'epoch': 4.84}\n","{'loss': 0.3082, 'grad_norm': 6.605960369110107, 'learning_rate': 1.7914285714285715e-06, 'epoch': 4.87}\n","{'loss': 0.3042, 'grad_norm': 5.9556803703308105, 'learning_rate': 1.72e-06, 'epoch': 4.91}\n","{'loss': 0.3026, 'grad_norm': 6.418491363525391, 'learning_rate': 1.6485714285714289e-06, 'epoch': 4.95}\n","{'loss': 0.3233, 'grad_norm': 5.910842418670654, 'learning_rate': 1.5771428571428574e-06, 'epoch': 4.98}\n","{'loss': 0.2972, 'grad_norm': 5.649760723114014, 'learning_rate': 1.5057142857142858e-06, 'epoch': 5.02}\n","{'loss': 0.2447, 'grad_norm': 5.27011251449585, 'learning_rate': 1.4342857142857144e-06, 'epoch': 5.05}\n","{'loss': 0.2629, 'grad_norm': 5.057453155517578, 'learning_rate': 1.362857142857143e-06, 'epoch': 5.09}\n","{'loss': 0.2709, 'grad_norm': 5.582757472991943, 'learning_rate': 1.2914285714285716e-06, 'epoch': 5.13}\n","{'loss': 0.2582, 'grad_norm': 4.89450740814209, 'learning_rate': 1.2200000000000002e-06, 'epoch': 5.16}\n","{'loss': 0.2611, 'grad_norm': 5.051322937011719, 'learning_rate': 1.1485714285714286e-06, 'epoch': 5.2}\n","{'loss': 0.2687, 'grad_norm': 5.4032182693481445, 'learning_rate': 1.0771428571428574e-06, 'epoch': 5.23}\n","{'loss': 0.2883, 'grad_norm': 5.636666774749756, 'learning_rate': 1.0057142857142857e-06, 'epoch': 5.27}\n","{'loss': 0.2773, 'grad_norm': 5.74999475479126, 'learning_rate': 9.342857142857144e-07, 'epoch': 5.31}\n","{'loss': 0.2591, 'grad_norm': 4.493468284606934, 'learning_rate': 8.628571428571429e-07, 'epoch': 5.34}\n","{'loss': 0.2788, 'grad_norm': 6.209619522094727, 'learning_rate': 7.914285714285715e-07, 'epoch': 5.38}\n","{'loss': 0.2912, 'grad_norm': 7.2936320304870605, 'learning_rate': 7.2e-07, 'epoch': 5.42}\n","{'loss': 0.2665, 'grad_norm': 5.304265975952148, 'learning_rate': 6.485714285714287e-07, 'epoch': 5.45}\n","{'loss': 0.2699, 'grad_norm': 6.076085090637207, 'learning_rate': 5.771428571428572e-07, 'epoch': 5.49}\n","{'loss': 0.2786, 'grad_norm': 4.554596424102783, 'learning_rate': 5.057142857142858e-07, 'epoch': 5.52}\n","{'loss': 0.2911, 'grad_norm': 5.789795398712158, 'learning_rate': 4.342857142857143e-07, 'epoch': 5.56}\n","{'loss': 0.2728, 'grad_norm': 6.435058116912842, 'learning_rate': 3.6285714285714283e-07, 'epoch': 5.6}\n","{'loss': 0.2786, 'grad_norm': 5.68804407119751, 'learning_rate': 2.914285714285715e-07, 'epoch': 5.63}\n","{'loss': 0.2692, 'grad_norm': 5.499497413635254, 'learning_rate': 2.2e-07, 'epoch': 5.67}\n","{'loss': 0.2836, 'grad_norm': 7.534801483154297, 'learning_rate': 1.4857142857142857e-07, 'epoch': 5.7}\n","{'loss': 0.2741, 'grad_norm': 6.170979976654053, 'learning_rate': 7.714285714285715e-08, 'epoch': 5.74}\n","{'loss': 0.2841, 'grad_norm': 5.962027072906494, 'learning_rate': 5.714285714285715e-09, 'epoch': 5.78}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c5111efab5a4c45915e03fde52fcc53","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/965 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\n"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.687467098236084, 'eval_wer': 54.15331344163259, 'eval_runtime': 1656.7347, 'eval_samples_per_second': 4.656, 'eval_steps_per_second': 0.582, 'epoch': 5.78}\n"]},{"name":"stderr","output_type":"stream","text":["There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 22943.0769, 'train_samples_per_second': 5.579, 'train_steps_per_second': 0.174, 'train_loss': 0.5698619546890259, 'epoch': 5.78}\n"]},{"data":{"text/plain":["TrainOutput(global_step=4000, training_loss=0.5698619546890259, metrics={'train_runtime': 22943.0769, 'train_samples_per_second': 5.579, 'train_steps_per_second': 0.174, 'total_flos': 8.300458008576e+18, 'train_loss': 0.5698619546890259, 'epoch': 5.776173285198556})"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()\n","#trainer.train(resume_from_checkpoint=True)"]},{"cell_type":"code","execution_count":34,"id":"0438189f","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eace045db165410585c1884a5d339e70","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/965 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 0.687467098236084,\n"," 'eval_wer': 54.15331344163259,\n"," 'eval_runtime': 1506.2654,\n"," 'eval_samples_per_second': 5.121,\n"," 'eval_steps_per_second': 0.641,\n"," 'epoch': 5.776173285198556}"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate(language=\"cs\")"]},{"cell_type":"code","execution_count":4,"id":"4c9751b6","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","whisper-bs-cs-train-noaug-test-noaug 4000 steps performance:\n","'eval_loss': 0.3769497871398926, 'eval_wer': 35.114377279257376, 'eval_runtime': 1471.0067, 'eval_samples_per_second': 5.244, 'eval_steps_per_second': 0.656, 'epoch': 5.78\n","'train_runtime': 20830.4373, 'train_samples_per_second': 6.145, 'train_steps_per_second': 0.192, 'train_loss': 0.25434225314855574, 'epoch': 5.78\n","\n","\n","whisper-base-cs-cv11-train-aug-test-aug 4000 steps performance:      \n","'eval_loss': 0.687467098236084, 'eval_wer': 54.15331344163259, 'eval_runtime': 1656.7347, 'eval_samples_per_second': 4.656, 'eval_steps_per_second': 0.582, 'epoch': 5.78\n","'train_runtime': 22943.0769, 'train_samples_per_second': 5.579, 'train_steps_per_second': 0.174, 'train_loss': 0.5698619546890259, 'epoch': 5.78\n","\n","\n","whisper-base-cs-cv11-train-stretch20-gain10-pitch20-gaussian20-lowpass10-toest-noaug 4000 steps performance:\n","'eval_loss': 0.3659650683403015, 'eval_wer': 35.235937672671014, 'eval_runtime': 1270.0477, 'eval_samples_per_second': 6.074, 'eval_steps_per_second': 0.76, 'epoch': 5.78\n","'train_runtime': 17820.5123, 'train_samples_per_second': 7.183, 'train_steps_per_second': 0.224, 'train_loss': 0.4966413743495941, 'epoch': 5.78\n","\n","\n","whisper-base-cs-cv11-train-noaug-test-timestretch20-gain10-pitch20-gaussian20-lowpass01-mp3compression01 4000 steps performance:\n","'eval_loss': 1.0829871892929077, 'eval_wer': 65.93546248204221, 'eval_runtime': 8263.055, 'eval_samples_per_second': 0.934, 'eval_steps_per_second': 0.117, 'epoch': 5.78\n","'train_runtime': 76764.5105, 'train_samples_per_second': 1.667, 'train_steps_per_second': 0.052, 'train_loss': 0.2543431313931942, 'epoch': 5.78\n","\n"]}],"source":["print(\"\"\"\n","whisper-bs-cs-train-noaug-test-noaug 4000 steps performance:\n","'eval_loss': 0.3769497871398926, 'eval_wer': 35.114377279257376, 'eval_runtime': 1471.0067, 'eval_samples_per_second': 5.244, 'eval_steps_per_second': 0.656, 'epoch': 5.78\n","'train_runtime': 20830.4373, 'train_samples_per_second': 6.145, 'train_steps_per_second': 0.192, 'train_loss': 0.25434225314855574, 'epoch': 5.78\n","\"\"\")\n","\n","print(\"\"\"\n","whisper-base-cs-cv11-train-aug-test-aug 4000 steps performance:      \n","'eval_loss': 0.687467098236084, 'eval_wer': 54.15331344163259, 'eval_runtime': 1656.7347, 'eval_samples_per_second': 4.656, 'eval_steps_per_second': 0.582, 'epoch': 5.78\n","'train_runtime': 22943.0769, 'train_samples_per_second': 5.579, 'train_steps_per_second': 0.174, 'train_loss': 0.5698619546890259, 'epoch': 5.78\n","\"\"\")\n","\n","print(\"\"\"\n","whisper-base-cs-cv11-train-stretch20-gain10-pitch20-gaussian20-lowpass10-toest-noaug 4000 steps performance:\n","'eval_loss': 0.3659650683403015, 'eval_wer': 35.235937672671014, 'eval_runtime': 1270.0477, 'eval_samples_per_second': 6.074, 'eval_steps_per_second': 0.76, 'epoch': 5.78\n","'train_runtime': 17820.5123, 'train_samples_per_second': 7.183, 'train_steps_per_second': 0.224, 'train_loss': 0.4966413743495941, 'epoch': 5.78\n","\"\"\"\n",")\n","\n","print(\"\"\"\n","whisper-base-cs-cv11-train-noaug-test-timestretch20-gain10-pitch20-gaussian20-lowpass01-mp3compression01 4000 steps performance:\n","'eval_loss': 1.0829871892929077, 'eval_wer': 65.93546248204221, 'eval_runtime': 8263.055, 'eval_samples_per_second': 0.934, 'eval_steps_per_second': 0.117, 'epoch': 5.78\n","'train_runtime': 76764.5105, 'train_samples_per_second': 1.667, 'train_steps_per_second': 0.052, 'train_loss': 0.2543431313931942, 'epoch': 5.78\n","\"\"\")"]},{"cell_type":"code","execution_count":38,"id":"d7030622-caf7-4039-939b-6195cdaa2585","metadata":{"id":"d7030622-caf7-4039-939b-6195cdaa2585"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"324e2975e6de4ac7ba8f4933d8a10012","version_major":2,"version_minor":0},"text/plain":["events.out.tfevents.1714514040.asus-lv.49669.1:   0%|          | 0.00/406 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/LadislavVasina1/whisper-bs-cs-train-aug-test-aug2/commit/03ff3564f5ad273982ea93a151d658878e0da4df', commit_message='End of training', commit_description='', oid='03ff3564f5ad273982ea93a151d658878e0da4df', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["trainer.push_to_hub()"]},{"cell_type":"markdown","id":"34d4360d-5721-426e-b6ac-178f833fedeb","metadata":{"id":"34d4360d-5721-426e-b6ac-178f833fedeb"},"source":["## Building a Demo"]},{"cell_type":"code","execution_count":39,"id":"e0ace3aa-1ef3-45cb-933f-6ddca037c5aa","metadata":{"id":"e0ace3aa-1ef3-45cb-933f-6ddca037c5aa"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68ed325a25c240baa35e51e61e40dc44","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.31k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9e93b3cef8df410082951fe69a84cbcb","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8b61a5f879774d6a970817b7e1524076","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/3.82k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02adf55c960544139b63d5c6f113c795","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"51df1ec1d6ca4f83805a2ff8409f17d1","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d38120cd3664cd886a5f0ebe474582a","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"55689ba224fe4b41af844e6be80e15b4","version_major":2,"version_minor":0},"text/plain":["normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9202deca6d6044da8b72eb1c23ec6991","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f63c7d6f63c44812a130897bd0f76c86","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad96ad9823ce4cc48ef0bbc0f82177fc","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/339 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Running on local URL:  http://127.0.0.1:7860\n","\n","To create a public link, set `share=True` in `launch()`.\n"]},{"data":{"text/html":["<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":[]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import pipeline\n","import gradio as gr\n","\n","\n","pipe = pipeline(model=f\"LadislavVasina1/{CURRENT_RUN_NAME}\")\n","\n","def transcribe(audio):\n","    text = pipe(audio)[\"text\"]\n","    return text\n","\n","gradio_app = gr.Interface(\n","    fn=transcribe,\n","    inputs=gr.Audio(type=\"filepath\"),\n","    outputs=\"text\",\n","    title=\"Whisper Base CS\",\n","    description=CURRENT_RUN_NAME,\n",")\n","\n","gradio_app.launch()"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb","timestamp":1702225079018}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}
